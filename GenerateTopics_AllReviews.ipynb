{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure out how to run lda on our text corpus. Along with this, please look at existing \n",
    "#examples to find how topic modeling is actually done in practice. You basically run \n",
    "#lda a 1000 times to get stable topics but I’ll leave that for your exercise. Your final\n",
    "#output should a list of top 20 topics with the top 20 responses for each topic. Let’s do \n",
    "#that for next wednesday and then Pam and I can individually,qualitatively take a look at \n",
    "#the topics and then discuss the following steps. \n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "#loading workbook\n",
    "wb=load_workbook('TeenSafetyApps_Reviews.xlsx')\n",
    "wb.get_sheet_names()\n",
    "sheet = wb.get_sheet_by_name('all_reviews')\n",
    "\n",
    "#print sheet details\n",
    "#sheet.title\n",
    "#type(sheet)\n",
    "#for x in range (2,29274):\n",
    "#    print(x,sheet.cell(row=x,column=5).value)\n",
    "\n",
    "#xstr(s) returns string when s is not none\n",
    "def xstr(s):\n",
    "    return '' if s is None else str(s)\n",
    "\n",
    "#select review text\n",
    "#x = input('Which review do you want me to clean (enter a row number beween 2 and 29273)?')\n",
    "# get content into text\n",
    "# get content into text\n",
    "text = \"\"\n",
    "for x in range(2,29274):\n",
    "    text += xstr(sheet.cell(row=x,column=5).value) + ' '   \n",
    "\n",
    "#text = \"\"\n",
    "#text = xstr(sheet.cell(row=int(x),column=5).value)    \n",
    "print(\"Raw review text:\")\n",
    "print(\"================\")\n",
    "print(text)\n",
    "\n",
    "#tokenizing text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_text = word_tokenize(text) \n",
    "print(\"Tokenized text:\")\n",
    "print(\"===============\")\n",
    "print(tokenized_text)\n",
    "\n",
    "#removing punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "\n",
    "tokenized_text_no_punctuation = []\n",
    "\n",
    "#for review in tokenized_text:\n",
    "new_review = []\n",
    "for token in tokenized_text: \n",
    "    new_token = regex.sub(u'', token)\n",
    "    if not new_token == u'':\n",
    "        new_review.append(new_token)\n",
    "    \n",
    "tokenized_text_no_punctuation.append(new_review)\n",
    "print(\"Tokenized text without punctuation:\")\n",
    "print(\"===================================\")    \n",
    "print(tokenized_text_no_punctuation)\n",
    "\n",
    "#cleaning stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_text_no_stopwords = []\n",
    "for doc in tokenized_text_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    tokenized_text_no_stopwords.append(new_term_vector)\n",
    "print(\"Text without stopwords:\")\n",
    "print(\"========================\")            \n",
    "print(tokenized_text_no_stopwords)\n",
    "\n",
    "##not using stemming\n",
    "\n",
    "##stemming/lemmatizing\n",
    "##from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "##from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "##porter = PorterStemmer()\n",
    "#snowball = SnowballStemmer('english')\n",
    "##wordnet = WordNetLemmatizer()\n",
    "\n",
    "#preprocessed_text = []\n",
    "\n",
    "#for doc in tokenized_text_no_stopwords:\n",
    "#    final_doc = [];\n",
    "#    for word in doc:\n",
    "#        #final_doc.append(porter.stem(word))\n",
    "#        final_doc.append(snowball.stem(word))\n",
    "#        #final_doc.append(wordnet.lemmatize(word)) \n",
    "#    preprocessed_text.append(final_doc)\n",
    "#print(\"Text after stemming:\")\n",
    "#print(\"====================\")  \n",
    "#print(preprocessed_text)\n",
    "#type(preprocessed_text)\n",
    "\n",
    "#creating clean text from tokenized_text_no_stopwords for gensim lda corpus\n",
    "text_clean = ' '.join(str(r) for v in tokenized_text_no_stopwords for r in v)\n",
    "print(text_clean)\n",
    "\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(tokenized_text_no_stopwords)\n",
    "#dictionary.save('/tmp/allreviews.dict')\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "#here new_vec is corpus\n",
    "new_vec = [dictionary.doc2bow(text_clean.lower().split())]\n",
    "print(new_vec)\n",
    "lda= models.LdaModel(new_vec, num_topics=20, id2word=dictionary, passes=100)\n",
    "#lda= models.LdaMulticore(new_vec, num_topics=20, id2word=dictionary, passes=100)\n",
    "lda.show_topics(num_words=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
